MMLU = [
    "mmlu_abstract_algebra",
    "mmlu_anatomy",
    "mmlu_astronomy",
    "mmlu_business_ethics",
    "mmlu_clinical_knowledge",
    "mmlu_college_biology",
    "mmlu_college_chemistry",
    "mmlu_college_computer_science",
    "mmlu_college_mathematics",
    "mmlu_college_medicine",
    "mmlu_college_physics",
    "mmlu_computer_security",
    "mmlu_conceptual_physics",
    "mmlu_econometrics",
    "mmlu_electrical_engineering",
    "mmlu_elementary_mathematics",
    "mmlu_formal_logic",
    "mmlu_global_facts",
    "mmlu_high_school_biology",
    "mmlu_high_school_chemistry",
    "mmlu_high_school_computer_science",
    "mmlu_high_school_european_history",
    "mmlu_high_school_geography",
    "mmlu_high_school_government_and_politics",
    "mmlu_high_school_macroeconomics",
    "mmlu_high_school_mathematics",
    "mmlu_high_school_microeconomics",
    "mmlu_high_school_physics",
    "mmlu_high_school_psychology",
    "mmlu_high_school_statistics",
    "mmlu_high_school_us_history",
    "mmlu_high_school_world_history",
    "mmlu_human_aging",
    "mmlu_human_sexuality",
    "mmlu_international_law",
    "mmlu_jurisprudence",
    "mmlu_logical_fallacies",
    "mmlu_machine_learning",
    "mmlu_management",
    "mmlu_marketing",
    "mmlu_medical_genetics",
    "mmlu_miscellaneous",
    "mmlu_moral_disputes",
    "mmlu_moral_scenarios",
    "mmlu_nutrition",
    "mmlu_philosophy",
    "mmlu_prehistory",
    "mmlu_professional_accounting",
    "mmlu_professional_law",
    "mmlu_professional_medicine",
    "mmlu_professional_psychology",
    "mmlu_public_relations",
    "mmlu_security_studies",
    "mmlu_sociology",
    "mmlu_us_foreign_policy",
    "mmlu_virology",
    "mmlu_world_religions",
]
MINERVA = [
    "minerva_math_algebra",
    "minerva_math_counting_and_probability",
    "minerva_math_geometry",
    "minerva_math_intermediate_algebra",
    "minerva_math_number_theory",
    "minerva_math_prealgebra",
    "minerva_math_precalculus",
]
MMLU_PRO = [
    "mmlu_pro_biology:rc",
    "mmlu_pro_business:rc",
    "mmlu_pro_chemistry:rc",
    "mmlu_pro_computer science:rc",
    "mmlu_pro_economics:rc",
    "mmlu_pro_engineering:rc",
    "mmlu_pro_health:rc",
    "mmlu_pro_history:rc",
    "mmlu_pro_law:rc",
    "mmlu_pro_math:rc",
    "mmlu_pro_other:rc",
    "mmlu_pro_philosophy:rc",
    "mmlu_pro_physics:rc",
    "mmlu_pro_psychology:rc",
]
OLMES = [
    "arc_challenge",
    "arc_easy",
    "boolq",
    "csqa",
    "hellaswag",
    "openbookqa",
    "piqa",
    "socialiqa",
    "winogrande",
]
OLMES_GEN = ["drop", "gsm8k", "jeopardy", "squad", "triviaqa"]  # naturalqs
AGI_EVAL = [
    "agi_eval_aqua-rat:rc",
    "agi_eval_gaokao-english:rc",
    "agi_eval_logiqa-en:rc",
    "agi_eval_lsat-ar:rc",
    "agi_eval_lsat-lr:rc"
    "agi_eval_lsat-rc:rc",
    "agi_eval_sat-en-without-passage:rc",
    "agi_eval_sat-en:rc",
    "agi_eval_sat-math:rc",
]
BBH = [
    "bbh_boolean_expressions",
    "bbh_causal_judgement",
    "bbh_date_understanding",
    "bbh_disambiguation_qa",
    "bbh_dyck_languages",
    "bbh_formal_fallacies",
    "bbh_geometric_shapes",
    "bbh_hyperbaton",
    "bbh_logical_deduction_five_objects",
    "bbh_logical_deduction_seven_objects",
    "bbh_logical_deduction_three_objects",
    "bbh_movie_recommendation",
    "bbh_multistep_arithmetic_two",
    "bbh_navigate",
    "bbh_object_counting",
    "bbh_penguins_in_a_table",
    "bbh_reasoning_about_colored_objects",
    "bbh_ruin_names",
    "bbh_salient_translation_error_detection",
    "bbh_snarks",
    "bbh_sports_understanding",
    "bbh_temporal_sequences",
    "bbh_tracking_shuffled_objects_five_objects",
    "bbh_tracking_shuffled_objects_seven_objects",
    "bbh_tracking_shuffled_objects_three_objects",
    "bbh_web_of_lies",
    "bbh_word_sorting",
]
MMLU_MC = [
    "mmlu_abstract_algebra:mc",
    "mmlu_anatomy:mc",
    "mmlu_astronomy:mc",
    "mmlu_business_ethics:mc",
    "mmlu_clinical_knowledge:mc",
    "mmlu_college_biology:mc",
    "mmlu_college_chemistry:mc",
    "mmlu_college_computer_science:mc",
    "mmlu_college_mathematics:mc",
    "mmlu_college_medicine:mc",
    "mmlu_college_physics:mc",
    "mmlu_computer_security:mc",
    "mmlu_conceptual_physics:mc",
    "mmlu_econometrics:mc",
    "mmlu_electrical_engineering:mc",
    "mmlu_elementary_mathematics:mc",
    "mmlu_formal_logic:mc",
    "mmlu_global_facts:mc",
    "mmlu_high_school_biology:mc",
    "mmlu_high_school_chemistry:mc",
    "mmlu_high_school_computer_science:mc",
    "mmlu_high_school_european_history:mc",
    "mmlu_high_school_geography:mc",
    "mmlu_high_school_government_and_politics:mc",
    "mmlu_high_school_macroeconomics:mc",
    "mmlu_high_school_mathematics:mc",
    "mmlu_high_school_microeconomics:mc",
    "mmlu_high_school_physics:mc",
    "mmlu_high_school_psychology:mc",
    "mmlu_high_school_statistics:mc",
    "mmlu_high_school_us_history:mc",
    "mmlu_high_school_world_history:mc",
    "mmlu_human_aging:mc",
    "mmlu_human_sexuality:mc",
    "mmlu_international_law:mc",
    "mmlu_jurisprudence:mc",
    "mmlu_logical_fallacies:mc",
    "mmlu_machine_learning:mc",
    "mmlu_management:mc",
    "mmlu_marketing:mc",
    "mmlu_medical_genetics:mc",
    "mmlu_miscellaneous:mc",
    "mmlu_moral_disputes:mc",
    "mmlu_moral_scenarios:mc",
    "mmlu_nutrition:mc",
    "mmlu_philosophy:mc",
    "mmlu_prehistory:mc",
    "mmlu_professional_accounting:mc",
    "mmlu_professional_law:mc",
    "mmlu_professional_medicine:mc",
    "mmlu_professional_psychology:mc",
    "mmlu_public_relations:mc",
    "mmlu_security_studies:mc",
    "mmlu_sociology:mc",
    "mmlu_us_foreign_policy:mc",
    "mmlu_virology:mc",
    "mmlu_world_religions:mc",
]
OLMES_MC = [
    "arc_challenge:mc",
    "arc_easy:mc",
    "boolq:mc",
    "csqa:mc",
    "hellaswag:mc",
    "openbookqa:mc",
    "piqa:mc",
    "socialiqa:mc",
    "winogrande:mc",
]
PALOMA = [
    "paloma_4chan_meta_sep",
    "paloma_c4_100_domains",
    "paloma_c4_en",
    "paloma_dolma-v1_5",
    "paloma_dolma_100_programing_languages",
    "paloma_dolma_100_subreddits",
    "paloma_falcon-refinedweb",
    "paloma_gab",
    "paloma_m2d2_s2orc_unsplit",
    "paloma_m2d2_wikipedia_unsplit",
    "paloma_manosphere_meta_sep",
    "paloma_mc4",
    "paloma_ptb",
    "paloma_redpajama",
    "paloma_twitterAAE_HELM_fixed",
    "paloma_wikitext_103",
]
CUSTOM_LOSS = ["custom_loss_numia_math", "custom_loss_sky_t1", "custom_loss_tulu_if"]

# task suites
MULTITASK_MATH = [
    "gsm_plus",
    "gsm_symbolic_main",
    "gsm_symbolic_p1",
    "gsm_symbolic_p2",
    "minerva_math_500",
    "aime",
]  # 6
MULTITASK_CODE = ["mbpp", "mbppplus", "codex_humaneval", "codex_humanevalplus"]  # 4
MULTITASK_KNOWLEDGE = (
    ["medmcqa", "autobencher"] + OLMES + MMLU + OLMES_GEN + MMLU_PRO + AGI_EVAL
)  # 19
MULTITASK = MULTITASK_KNOWLEDGE + MULTITASK_MATH + MULTITASK_CODE + BBH  # 30
OLMES_ALL = OLMES + MMLU + OLMES_GEN

# Re-order tasks so that the title logic works (a bit hacky, yes)
OLMES_ALL = ["jeopardy"] + list(set(OLMES_ALL) - {"jeopardy"})
MULTITASK = ["boolq"] + list(set(MULTITASK) - {"boolq"})
